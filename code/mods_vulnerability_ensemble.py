import numpy as np


def extract_vulnerability_parameters_from(vulnerability):
    from tqdm import tqdm

    print("manage dictionary and clean up ...")
    # https://github.com/37stu37/hyperedges_paper_2022/blob/main/pre-processing_bldg_w_vulnerability.ipynb?short_path=a4a8fcc
    type_idx = []
    type_cnt = []
    type_size = []

    idx = 0

    # Change string to dictionary
    for (c, s) in zip(vulnerability.cnt_dist, vulnerability.size_dist):

        if c.endswith(('}')):
            d = eval(c)
            type_cnt.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", c)
            d = eval(c.rpartition(', "')[0] + ' }')
            type_cnt.append(d)

        if s.endswith(('}')):
            d = eval(s)
            type_size.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", s)
            d = eval(s.rpartition(', "')[0] + ' }')
            type_size.append(d)
            idx += 1

        type_idx.append(idx)

    # Add dictionaries to vulnearbility dataframe
    vulnerability['c_type'] = type_cnt
    vulnerability['c_size'] = type_size
    
    # extract number of buildings and types
    nbr_bldg = []
    type_bldg = []

    print('extracting building typologies and counts :')
    for s in tqdm(type_cnt):
        # d = eval(s)
        nbr_bldg.append(list(s.values()))
        type_bldg.append(list(s.keys()))

    return nbr_bldg, type_bldg


def probability_of_collapse_mean_parameters(type_bldg, nbr_bldg, vulnerabilities):
    from tqdm import tqdm
    '''
    for reference on the vulnerability functions
    https://docs.google.com/spreadsheets/d/1s0kjr6Em-equdCFOGjzq8k8Ht2Rb6WfjjkTBxITOJw0/edit#gid=1265400908
    '''
    C99mean = np.array([0.4295, 1.29, 1.95])
    C99stddev = np.array([0.3056, 0.83, 0.71])

    MURmean = np.array([0.23, 1.26, 1.9])
    MURstddev = np.array([0.31, 1.96, 0.93])

    MUR_STRUBmean = np.array([0.203, 0.39, 1.26])
    MUR_STRUBstddev = np.array([0.308, 0.69, 1.96])

    Smean = np.array([0.3682, 0.4664, 0.7364])
    Sstddev = np.array([0.262, 0.3318, 0.5239])

    Wmean = np.array([0.945, 1.1659, 1.6446])
    Wstddev = np.array([0.64, 0.8295, 1.1701])

    #create list to record weighted mean and std
    mmean_low=[]
    mmean_medium=[]
    mmean_high=[]
    mstd_low=[]
    mstd_medium=[]
    mstd_high=[]
    
    # iterate type_bldg and nbr_bldg list of lists for each METEOR cell
    for ty, ct in tqdm(zip(type_bldg, nbr_bldg)):
        
        mean_low=[]
        mean_medium=[]
        mean_high=[]
        std_low=[]
        std_medium=[]
        std_high=[]
        count=[]
        
        # iterate lists
        for t, c in zip(ty, ct):
            
            if t.startswith('C99'):
                st_low, m_low = C99stddev[2], C99mean[2]
                st_medium, m_medium = C99stddev[1], C99mean[1]
                st_high, m_high = C99stddev[0], C99mean[0]

            elif (t.startswith('MUR+STRUB')) or (t.startswith('MATO')):
                st_low, m_low = MUR_STRUBstddev[2], MUR_STRUBmean[2]
                st_medium, m_medium = MUR_STRUBstddev[1], MUR_STRUBmean[1]
                st_high, m_high = MUR_STRUBstddev[0], MUR_STRUBmean[0]

            elif (t.startswith('MUR')) and not (('MUR+STRUB' in t)):
                st_low, m_low = MURstddev[2], MURmean[2]
                st_medium, m_medium = MURstddev[1], MURmean[1]
                st_high, m_high = MURstddev[0], MURmean[0]

            elif t.startswith('S'):
                st_low, m_low = Sstddev[2], Smean[2]
                st_medium, m_medium = Sstddev[1], Smean[1]
                st_high, m_high = Sstddev[0], Smean[0]

            elif t.startswith('W'):
                st_low, m_low = Wstddev[2], Wmean[2]
                st_medium, m_medium = Wstddev[1], Wmean[1]
                st_high, m_high = Wstddev[0], Wmean[0]

            else:
                print('there is a problem here', ty)

            mean_low.append(m_low)
            mean_medium.append(m_medium)
            mean_high.append(m_high)
            std_low.append(st_low)
            std_medium.append(st_medium)
            std_high.append(st_high)
            count.append(c)
            
        # calculate weighted mean
        mmean_low.append(np.mean(np.repeat(mean_low, count)))
        mmean_medium.append(np.mean(np.repeat(mean_medium, count)))
        mmean_high.append(np.mean(np.repeat(mean_high, count)))
        mstd_low.append(np.mean(np.repeat(std_low, count)))
        mstd_medium.append(np.mean(np.repeat(std_medium, count)))
        mstd_high.append(np.mean(np.repeat(std_high, count)))

    vulnerabilities['mmean_low'], vulnerabilities['mmean_medium'], vulnerabilities['mmean_high'] = mmean_low, mmean_medium, mmean_high
    vulnerabilities['mstd_low'], vulnerabilities['mstd_medium'], vulnerabilities['mstd_high'] = mstd_low, mstd_medium, mstd_high

    return vulnerabilities


def join_buildings_and_vulnerabilities(buildings, vulnerabilities):
    import geopandas as gpd
    print("joining vulnerability to building stock ...")
    buildings = gpd.sjoin_nearest(buildings, vulnerabilities[['mmean_low', 'mmean_medium', 'mmean_high', 'mstd_low', 'mstd_medium', 'mstd_high', 'geometry']], 
                          distance_col="distances", how="left")
    buildings = buildings.sort_values(by=['distances'])
    buildings.drop_duplicates(subset=['osm_id'], keep="first", inplace=True)
    
    return buildings


def calculate_probability_of_collapse(buildings):
    # from scipy.stats import lognorm
    from numba_stats import lognorm
    from tqdm import trange
    
    high = [lognorm.cdf(buildings['pgas'].values[i], s=buildings['mstd_high'].values[i], loc=0, scale=buildings['mmean_high'].values[i]) for i in trange(buildings.shape[0])]
    medium = [lognorm.cdf(buildings['pgas'].values[i], s=buildings['mstd_medium'].values[i], loc=0, scale=buildings['mmean_medium'].values[i]) for i in trange(buildings.shape[0])]
    low = [lognorm.cdf(buildings['pgas'].values[i], s=buildings['mstd_low'].values[i], loc=0, scale=buildings['mmean_low'].values[i]) for i in trange(buildings.shape[0])]
    
    # manual correction for fragility curve crossing at low pgas
    high = np.maximum(np.maximum(high, medium), low)
    low = np.minimum(np.minimum(high, medium), low)
    
    return high, medium, low
