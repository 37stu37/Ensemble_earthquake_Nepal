from numba import njit
import numpy as np
from scipy.stats import lognorm


def append_vulnerability(buildings, vulnerability):
    import geopandas as gpd
    from tqdm import tqdm

    print("manage dictionary and clean up ...")
    # https://github.com/37stu37/hyperedges_paper_2022/blob/main/pre-processing_bldg_w_vulnerability.ipynb?short_path=a4a8fcc
    type_idx = []
    type_cnt = []
    type_size = []

    idx = 0

    # Change string to dictionary
    for (c, s) in zip(vulnerability.cnt_dist, vulnerability.size_dist):

        if c.endswith(('}')):
            d = eval(c)
            type_cnt.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", c)
            d = eval(c.rpartition(', "')[0] + ' }')
            type_cnt.append(d)

        if s.endswith(('}')):
            d = eval(s)
            type_size.append(d)
        else:  # for incomplete dictionary remove invalid part of the string starting from the end
            # print(f"idx = {idx}", s)
            d = eval(s.rpartition(', "')[0] + ' }')
            type_size.append(d)
            idx += 1

        type_idx.append(idx)

    # Add dictionaries to vulnearbility dataframe
    vulnerability['c_type'] = type_cnt
    vulnerability['c_size'] = type_size

    print("joining vulnerability to building stock ...")
    b = gpd.sjoin_nearest(buildings, vulnerability[['c_type', 'c_size', 'geometry']], distance_col="distances",
                          how="left")
    b = b.sort_values(by=['distances'])
    b.drop_duplicates(subset=['osm_id'], keep="first", inplace=True)

    print("extracting typology and count ...")
    # extract number of buildings and types
    nbr_bldg = []
    type_bldg = []

    print('extracting building typologies and counts :')
    for s in tqdm(b.c_type):
        # d = eval(s)
        nbr_bldg.append(list(s.values()))
        type_bldg.append(list(s.keys()))

    return nbr_bldg, type_bldg


# @njit
def weighted_probability_of_collapse(type_bldg, nbr_bldg, pgas):
    from tqdm import tqdm

    '''
    for reference on the vulnerability functions
    https://docs.google.com/spreadsheets/d/1s0kjr6Em-equdCFOGjzq8k8Ht2Rb6WfjjkTBxITOJw0/edit#gid=1265400908
    '''
    C99mean = np.array([0.4295, 1.29, 1.95])
    C99stddev = np.array([0.3056, 0.83, 0.71])

    MURmean = np.array([0.23, 1.26, 1.9])
    MURstddev = np.array([0.31, 1.96, 0.93])

    MUR_STRUBmean = np.array([0.203, 0.39, 1.26])
    MUR_STRUBstddev = np.array([0.308, 0.69, 1.96])

    Smean = np.array([0.3682, 0.4664, 0.7364])
    Sstddev = np.array([0.262, 0.3318, 0.5239])

    Wmean = np.array([0.945, 1.1659, 1.6446])
    Wstddev = np.array([0.64, 0.8295, 1.1701])

    # sample individual building and calculate weighted mean low, mid, high fragilities
    low_case_probability = np.zeros(len(type_bldg))
    mid_case_probability = np.zeros(len(type_bldg))
    high_case_probability = np.zeros(len(type_bldg))

    print('sampling building fragilities :')  #
    for i, _ in tqdm(enumerate(type_bldg)):

        ll = np.zeros(len(type_bldg[i]))
        lm = np.zeros(len(type_bldg[i]))
        lh = np.zeros(len(type_bldg[i]))
        c = np.zeros(len(nbr_bldg[i]))

        for j, _ in enumerate(type_bldg[i]):

            typology = type_bldg[i][j]
            count = nbr_bldg[i][j]

            if typology.startswith('C99'):
                high = lognorm(C99stddev[0], scale=C99mean[0]).cdf(pgas[i])
                mid = lognorm(C99stddev[1], scale=C99mean[1]).cdf(pgas[i])
                low = lognorm(C99stddev[2], scale=C99mean[2]).cdf(pgas[i])

            elif (typology.startswith('MUR+STRUB')) or (typology.startswith('MATO')):
                high = lognorm(MUR_STRUBstddev[0], scale=MUR_STRUBmean[0]).cdf(pgas[i])
                mid = lognorm(MUR_STRUBstddev[1], scale=MUR_STRUBmean[1]).cdf(pgas[i])
                low = lognorm(MUR_STRUBstddev[2], scale=MUR_STRUBmean[2]).cdf(pgas[i])

            elif (typology.startswith('MUR')) and not (('MUR+STRUB' in typology)):
                high = lognorm(MURstddev[0], scale=MURmean[0]).cdf(pgas[i])
                mid = lognorm(MURstddev[1], scale=MURmean[1]).cdf(pgas[i])
                low = lognorm(MURstddev[2], scale=MURmean[2]).cdf(pgas[i])

            elif typology.startswith('S'):
                high = lognorm(Sstddev[0], scale=Smean[0]).cdf(pgas[i])
                mid = lognorm(Sstddev[1], scale=Smean[1]).cdf(pgas[i])
                low = lognorm(Sstddev[2], scale=Smean[2]).cdf(pgas[i])

            elif typology.startswith('W'):
                high = lognorm(Wstddev[0], scale=Wmean[0]).cdf(pgas[i])
                mid = lognorm(Wstddev[1], scale=Wmean[1]).cdf(pgas[i])
                low = lognorm(Wstddev[2], scale=Wmean[2]).cdf(pgas[i])

            else:
                print('there is a problem here', i)

            # append to lists
            ll[j] = low
            lm[j] = mid
            lh[j] = high
            c[j] = count

        # repeat fragility probability to weight the impact
        weighted_ll = np.mean(np.repeat(ll, count))
        weighted_lm = np.mean(np.repeat(lm, count))
        weighted_lh = np.mean(np.repeat(lh, count))

        # append mean probability to list
        low_case_probability[i] = weighted_ll
        mid_case_probability[i] = weighted_lm
        high_case_probability[i] = weighted_lh

    return low_case_probability, mid_case_probability, high_case_probability
